# -*- coding: utf-8 -*-
"""hackerearth_friendshipGoal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T3IVJT8WEiu_bgXbEjDNZr6RjRIStxtW

# **HackerEarth Deep Learning Challenge: #FriendshipGoals**

Contest link: [https://www.hackerearth.com/challenges/competitive/hackerearth-deep-learning-challenge-friendship-day/?utm_source=challenges-modern&utm_campaign=participated-challenges&utm_medium=right-panel](https://www.hackerearth.com/challenges/competitive/hackerearth-deep-learning-challenge-friendship-day/?utm_source=challenges-modern&utm_campaign=participated-challenges&utm_medium=right-panel)

## **Mounting drive on Collab**
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Download dataset in drive directly from Kaggle**"""

# Commented out IPython magic to ensure Python compatibility.
import os
os.environ['KAGGLE_CONFIG_DIR'] = "/content/drive/My Drive/kaggle"

#changing the working directory
# %cd /content/drive/My Drive/kaggle

!kaggle datasets download -d bing101/friendshipgoals

"""### **Or Download Dataset manually from here**
[https://www.kaggle.com/bing101/friendshipgoals/download](https://www.kaggle.com/bing101/friendshipgoals/download)

## **Unzipping Dataset**
"""

!unzip \*.zip  && rm *.zip

"""## **Oh Libraries! What are we without you**"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import shutil, random

from tensorflow.keras.applications.resnet_v2 import ResNet50V2
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, GlobalAveragePooling2D, GaussianNoise

from tensorflow.keras.optimizers import Adam
from tensorflow.keras import models, regularizers, layers, optimizers, losses, metrics
from tensorflow.python.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet_v2 import preprocess_input

image_size = 224
num_classes = 3

train_path = '/content/drive/My Drive/kaggle/data/train'
validate_path = '/content/drive/My Drive/kaggle/data/validate'
test_path = '/content/drive/My Drive/kaggle/data/test'

"""## **Train - Validate Split**
### Given kaggle dataset has no separate validation images folder. Transfering ~20% of train images classwise to validation folder
"""

for name in ["/Toddler/", "/Teenagers/", "/Adults/"]:

  source = train_path + name
  dest = validate_path + name

  images = os.listdir(source)
  print(len(images))

  for image in images[:200]:
      shutil.move(source+image, dest+image)

"""## **Image Data Generator**"""

data_generator_train = ImageDataGenerator( preprocessing_function= preprocess_input,
                                           horizontal_flip = True,
                                           width_shift_range = 0.2,
                                           height_shift_range = 0.2,
                                           rotation_range = 10
                                          )
            
data_generator_validate = ImageDataGenerator(preprocessing_function= preprocess_input)

data_generator_test = ImageDataGenerator(preprocessing_function= preprocess_input)

train_generator = data_generator_train.flow_from_directory(
        directory = train_path,
        shuffle=True,
        target_size=(image_size, image_size),
        batch_size= 16,
        class_mode='categorical',
        )

validation_generator = data_generator_validate.flow_from_directory(
        directory = validate_path,
        shuffle=True,
        target_size=(image_size, image_size),
        batch_size = 16,
        class_mode='categorical',
        )

test_generator = data_generator_test.flow_from_directory(
        directory = test_path,
        shuffle=False,
        target_size=(image_size, image_size),
        class_mode=None,
        )

"""## **Creating Model - with ResNet50V2 as Base Model**"""

#base Model
conv_base = ResNet50V2(weights='imagenet', include_top=False, input_shape = (image_size, image_size, 3))

for layer in conv_base.layers[:]:
  layer.trainable = False

my_model = Sequential()
my_model.add(conv_base)
my_model.add(GlobalAveragePooling2D())
my_model.add(Dense(64, activation='relu'))
my_model.add(Dropout(0.2))
my_model.add(Dense(32, activation='relu'))
my_model.add(Dropout(0.2))
my_model.add(Dense(num_classes, activation='softmax'))

my_model.compile(optimizer= Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])

"""## **Model Summary**"""

print(my_model.summary())

"""## **Fitting the Model**"""

history = my_model.fit(train_generator,
                        # steps_per_epoch=40,
                        epochs = 5,
                        validation_data=validation_generator
                      )

"""## **Plotting Accuracy and Loss**"""

acc      = history.history[ 'accuracy' ]
val_acc  = history.history[ 'val_accuracy' ]
loss     = history.history[ 'loss' ]
val_loss = history.history['val_loss' ]

epochs   = range(len(acc)) # Get number of epochs

#------------------------------------------------
# Plot training and validation accuracy per epoch
#------------------------------------------------
plt.plot( epochs, acc )
plt.plot( epochs, val_acc )
plt.title('Training and validation accuracy')
plt.figure()

#------------------------------------------------
# Plot training and validation loss per epoch
#------------------------------------------------
plt.plot  ( epochs,     loss )
plt.plot  ( epochs, val_loss )
plt.title ('Training and validation loss'   )

"""## **Fine-tuning last few layers of base ResNet50V2 Model**"""

for layer in conv_base.layers[:179]:
   layer.trainable = False
for layer in conv_base.layers[179:]:
   layer.trainable = True

for i, layer in enumerate(conv_base.layers):
   print(i, layer.name, layer.trainable)

# conv_base.layers[179].trainable = True
# conv_base.layers[180].trainable = False
# conv_base.layers[184].trainable = False

my_model.compile(optimizer= Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])

history = my_model.fit(train_generator,
                      #  steps_per_epoch=40,
                        epochs = 5,
                        validation_data=validation_generator
                      )

"""## **Label Encoding the Images**"""

lst = ['Adults','Teenagers','Toddler']
from sklearn.preprocessing import LabelEncoder

LE = LabelEncoder()
LE.fit(lst)

"""## **Model Prediction**"""

prediction = my_model.predict(test_generator)
result = np.argmax(prediction, axis=1)

img_names = os.listdir('/content/drive/My Drive/kaggle/data/test/test')

target = {}

for i,img in enumerate(img_names):
  target[img] = LE.inverse_transform([result[i]])[0]

result[:5]

img_names[:5]

"""## **Generate output.csv** - Containing predictions for given test images"""

labels = pd.read_csv('/content/drive/My Drive/kaggle/data/Test.csv')
img_names = labels.iloc[:,:].values.reshape(-1,)

final= []
for img in img_names:
  final.append([img, target[img]])

df = pd.DataFrame(final, columns = ["Filename","Category"])
output_csv = df.to_csv('./output.csv', index=False)

"""## **Analysis of output.csv**"""

kk = pd.read_csv('output.csv')

kk.head()

kk['Category'].value_counts().plot(kind='bar')